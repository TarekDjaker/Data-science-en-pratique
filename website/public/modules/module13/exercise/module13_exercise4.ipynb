{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEAUjjhOb136"
      },
      "source": [
        "### Run in collab\n",
        "<a href=\"https://colab.research.google.com/github/racousin/data_science_practice/blob/master/website/public/modules/module13/exercise/module13_exercise4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uVgWUZjpb137",
        "outputId": "359f98e8-35ba-4f2a-8931-feb073946a5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig==4.2.1 in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Requirement already satisfied: gymnasium==0.29.1 in /usr/local/lib/python3.11/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (4.13.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (0.0.4)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install swig==4.2.1\n",
        "!pip install gymnasium==0.29.1\n",
        "!pip install gymnasium[box2d]  # Install Box2D dependency for LunarLander-v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Oa03cAjLb138"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJZwAAf2b139"
      },
      "source": [
        "# module13_exercise4 : ML - Arena <a href=\"https://ml-arena.com/viewcompetition/1\" target=\"_blank\"> LunarLander</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYQQPZhpb139"
      },
      "source": [
        "### Objective\n",
        "Get at list an agent running on ML-Arena <a href=\"https://ml-arena.com/viewcompetition/1\" target=\"_blank\"> LunarLander</a> with mean reward upper than 50\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lCv3K7-8S-K"
      },
      "source": [
        "You should submit an agent file named `agent.py` with a class `Agent` that includes at least the following attributes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RLDC1kaM8S-K",
        "outputId": "b7f429df-94f1-43a5-fcb6-fe39007f6f37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "√âpisode 10/1000 - Score moyen (100 derniers): -176.6 - eps=0.951\n",
            "√âpisode 20/1000 - Score moyen (100 derniers): -170.4 - eps=0.905\n",
            "√âpisode 30/1000 - Score moyen (100 derniers): -164.4 - eps=0.860\n",
            "√âpisode 40/1000 - Score moyen (100 derniers): -157.8 - eps=0.818\n",
            "√âpisode 50/1000 - Score moyen (100 derniers): -150.5 - eps=0.778\n",
            "√âpisode 60/1000 - Score moyen (100 derniers): -150.8 - eps=0.740\n",
            "√âpisode 70/1000 - Score moyen (100 derniers): -155.2 - eps=0.704\n",
            "√âpisode 80/1000 - Score moyen (100 derniers): -152.7 - eps=0.670\n",
            "√âpisode 90/1000 - Score moyen (100 derniers): -148.2 - eps=0.637\n",
            "√âpisode 100/1000 - Score moyen (100 derniers): -144.3 - eps=0.606\n",
            "√âpisode 110/1000 - Score moyen (100 derniers): -139.2 - eps=0.576\n",
            "√âpisode 120/1000 - Score moyen (100 derniers): -135.4 - eps=0.548\n",
            "√âpisode 130/1000 - Score moyen (100 derniers): -128.1 - eps=0.521\n",
            "√âpisode 140/1000 - Score moyen (100 derniers): -124.9 - eps=0.496\n",
            "√âpisode 150/1000 - Score moyen (100 derniers): -119.8 - eps=0.471\n",
            "√âpisode 160/1000 - Score moyen (100 derniers): -112.0 - eps=0.448\n",
            "√âpisode 170/1000 - Score moyen (100 derniers): -94.9 - eps=0.427\n",
            "√âpisode 180/1000 - Score moyen (100 derniers): -88.7 - eps=0.406\n",
            "√âpisode 190/1000 - Score moyen (100 derniers): -81.0 - eps=0.386\n",
            "√âpisode 200/1000 - Score moyen (100 derniers): -76.9 - eps=0.367\n",
            "√âpisode 210/1000 - Score moyen (100 derniers): -67.1 - eps=0.349\n",
            "√âpisode 220/1000 - Score moyen (100 derniers): -60.3 - eps=0.332\n",
            "√âpisode 230/1000 - Score moyen (100 derniers): -57.6 - eps=0.316\n",
            "√âpisode 240/1000 - Score moyen (100 derniers): -50.3 - eps=0.300\n",
            "√âpisode 250/1000 - Score moyen (100 derniers): -44.7 - eps=0.286\n",
            "√âpisode 260/1000 - Score moyen (100 derniers): -42.6 - eps=0.272\n",
            "√âpisode 270/1000 - Score moyen (100 derniers): -43.3 - eps=0.258\n",
            "√âpisode 280/1000 - Score moyen (100 derniers): -43.1 - eps=0.246\n",
            "√âpisode 290/1000 - Score moyen (100 derniers): -39.2 - eps=0.234\n",
            "√âpisode 300/1000 - Score moyen (100 derniers): -31.1 - eps=0.222\n",
            "√âpisode 310/1000 - Score moyen (100 derniers): -29.6 - eps=0.211\n",
            "√âpisode 320/1000 - Score moyen (100 derniers): -26.4 - eps=0.201\n",
            "√âpisode 330/1000 - Score moyen (100 derniers): -19.2 - eps=0.191\n",
            "√âpisode 340/1000 - Score moyen (100 derniers): -20.1 - eps=0.182\n",
            "√âpisode 350/1000 - Score moyen (100 derniers): -19.7 - eps=0.173\n",
            "√âpisode 360/1000 - Score moyen (100 derniers): -15.3 - eps=0.165\n",
            "√âpisode 370/1000 - Score moyen (100 derniers): -13.2 - eps=0.157\n",
            "√âpisode 380/1000 - Score moyen (100 derniers): -8.6 - eps=0.149\n",
            "√âpisode 390/1000 - Score moyen (100 derniers): -10.7 - eps=0.142\n",
            "√âpisode 400/1000 - Score moyen (100 derniers): -7.9 - eps=0.135\n",
            "√âpisode 410/1000 - Score moyen (100 derniers): 0.9 - eps=0.128\n",
            "√âpisode 420/1000 - Score moyen (100 derniers): 13.3 - eps=0.122\n",
            "√âpisode 430/1000 - Score moyen (100 derniers): 24.2 - eps=0.116\n",
            "√âpisode 440/1000 - Score moyen (100 derniers): 42.4 - eps=0.110\n",
            "√âpisode 450/1000 - Score moyen (100 derniers): 55.3 - eps=0.105\n",
            "√âpisode 460/1000 - Score moyen (100 derniers): 73.6 - eps=0.100\n",
            "√âpisode 470/1000 - Score moyen (100 derniers): 90.9 - eps=0.095\n",
            "√âpisode 480/1000 - Score moyen (100 derniers): 113.9 - eps=0.090\n",
            "√âpisode 490/1000 - Score moyen (100 derniers): 134.1 - eps=0.086\n",
            "√âpisode 500/1000 - Score moyen (100 derniers): 149.8 - eps=0.082\n",
            "√âpisode 510/1000 - Score moyen (100 derniers): 162.9 - eps=0.078\n",
            "√âpisode 520/1000 - Score moyen (100 derniers): 173.8 - eps=0.074\n",
            "√âpisode 530/1000 - Score moyen (100 derniers): 183.4 - eps=0.070\n",
            "√âpisode 540/1000 - Score moyen (100 derniers): 187.1 - eps=0.067\n",
            "√âpisode 550/1000 - Score moyen (100 derniers): 197.0 - eps=0.063\n",
            "Environnement r√©solu en 559 √©pisodes üéâ  (score moyen sur 100 eps = 200.9)\n",
            "Meilleure moyenne obtenue sur 100 √©pisodes: 200.9263291666409\n",
            "R√©compense moyenne sur 100 √©pisodes d'√©valuation: 207.60\n",
            ">>> Performance cible atteinte! L'agent obtient en moyenne au-dessus de 200 ‚úÖ\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import gymnasium as gym  # n√©cessite gymnasium[box2d] pour LunarLander-v2\n",
        "\n",
        "# D√©finition du r√©seau de neurones pour approximer Q(s,a)\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        # R√©seau fully-connected avec 2 couches cach√©es de 128 neurones\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc_out = nn.Linear(128, action_dim)\n",
        "        # Initialisation optionnelle des poids peut √™tre ajout√©e ici si d√©sir√©\n",
        "\n",
        "    def forward(self, state):\n",
        "        # Passe avant : ReLU sur couches cach√©es\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc_out(x)  # sorties Q-values (une par action)\n",
        "\n",
        "# Buffer d'exp√©rience pour stocker et √©chantillonner des transitions\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity, state_dim):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []        # liste de transitions\n",
        "        self.position = 0       # index courant pour √©craser les anciennes exp√©riences\n",
        "        self.state_dim = state_dim\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        # Si la m√©moire n'est pas encore pleine, on ajoute une nouvelle entr√©e\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        # Stocker la transition (on copie les tableaux pour √©viter les r√©f√©rences)\n",
        "        self.memory[self.position] = (\n",
        "            np.array(state, copy=True),\n",
        "            action,\n",
        "            reward,\n",
        "            np.array(next_state, copy=True),\n",
        "            done\n",
        "        )\n",
        "        # Incr√©ment circulaire de la position\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # Tirer al√©atoirement batch_size transitions\n",
        "        indices = np.random.choice(len(self.memory), batch_size, replace=False)\n",
        "        states, actions, rewards, next_states, dones = zip(*(self.memory[i] for i in indices))\n",
        "        # Convertir en tenseurs PyTorch\n",
        "        states      = torch.tensor(np.array(states), dtype=torch.float32)\n",
        "        actions     = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)    # actions indices\n",
        "        rewards     = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)  # r√©compenses\n",
        "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32)\n",
        "        dones       = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)    # indicateurs de fin (0.0 ou 1.0)\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# Agent DQN avec r√©seau local et r√©seau cible\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        # Initialiser les deux r√©seaux (policy et target) et l'optimiseur\n",
        "        self.q_network = QNetwork(state_dim, action_dim)\n",
        "        self.target_network = QNetwork(state_dim, action_dim)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())  # initialisation identique\n",
        "        self.target_network.eval()  # le r√©seau cible n'est pas entra√Æn√© par gradient\n",
        "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=5e-4)\n",
        "        # Initialiser la m√©moire d'exp√©rience\n",
        "        self.memory = ReplayBuffer(capacity=100000, state_dim=state_dim)\n",
        "        # Compteur de pas pour gestion des mises √† jour\n",
        "        self.learn_step_counter = 0\n",
        "\n",
        "    def select_action(self, state, epsilon):\n",
        "        \"\"\"Renvoie une action selon une politique epsilon-greedy.\"\"\"\n",
        "        if np.random.rand() < epsilon:\n",
        "            # Exploration al√©atoire\n",
        "            return np.random.randint(self.action_dim)\n",
        "        else:\n",
        "            # Exploitation (on choisit l'action de Q maximale)\n",
        "            state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # shape (1, state_dim)\n",
        "            self.q_network.eval()  # mode √©valuation\n",
        "            with torch.no_grad():\n",
        "                q_values = self.q_network(state_t)\n",
        "            self.q_network.train()  # repasse en mode entra√Ænement\n",
        "            action = int(torch.argmax(q_values, dim=1).item())\n",
        "            return action\n",
        "\n",
        "    def train_step(self, batch_size=64, gamma=0.99, tau=1e-3):\n",
        "        \"\"\"Effectue un pas d'apprentissage du r√©seau (une mise √† jour de Q-network).\"\"\"\n",
        "        if len(self.memory) < batch_size:\n",
        "            return  # ne pas entra√Æner tant qu'on n'a pas assez d'√©chantillons\n",
        "        # √âchantillonner un mini-batch de transitions\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(batch_size)\n",
        "        # Calcul des Q-cibles avec le r√©seau cible (on ne calcule pas de gradients ici)\n",
        "        with torch.no_grad():\n",
        "            # Valeur Q max du prochain √©tat selon le r√©seau cible\n",
        "            q_next = self.target_network(next_states).max(dim=1, keepdim=True)[0]\n",
        "            # Cible de Q: r + gamma * max(Q_next) * (1 - done)\n",
        "            q_target = rewards + gamma * q_next * (1 - dones)\n",
        "        # Valeur Q courante pr√©dite par le r√©seau principal pour les (state, action) du batch\n",
        "        q_current = self.q_network(states).gather(1, actions)  # Q(s,a) pour chaque transition du batch\n",
        "        # Calcul de la perte (erreur quadratique)\n",
        "        loss = F.mse_loss(q_current, q_target)\n",
        "        # R√©tropropagation de la perte et mise √† jour des poids du Q-network\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        # Mise √† jour douce du r√©seau cible vers le Q-network (tau)\n",
        "        for target_param, local_param in zip(self.target_network.parameters(), self.q_network.parameters()):\n",
        "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
        "\n",
        "# Environnement LunarLander-v2\n",
        "env = gym.make(\"LunarLander-v3\")\n",
        "state_dim = env.observation_space.shape[0]   # dimension d'√©tat (8)\n",
        "action_dim = env.action_space.n             # nombre d'actions (4)\n",
        "agent = DQNAgent(state_dim, action_dim)\n",
        "\n",
        "# Param√®tres d'entra√Ænement\n",
        "num_episodes = 1000         # nombre maximal d'√©pisodes\n",
        "max_steps = 1000            # pas max par √©pisode (pour √©viter des boucles infinies)\n",
        "target_score = 200          # score cible √† atteindre en moyenne\n",
        "print_interval = 10         # intervalle pour affichage des progr√®s\n",
        "best_avg_reward = -float(\"inf\")\n",
        "best_model_path = \"best_model.pth\"\n",
        "\n",
        "# Variables pour suivi de la performance\n",
        "scores = []                 # liste des scores par √©pisode\n",
        "scores_window = []          # fen√™tre glissante des derniers 100 scores\n",
        "\n",
        "# Boucle principale d'entra√Ænement\n",
        "epsilon = 1.0               # valeur initiale de epsilon (politique epsilon-greedy)\n",
        "epsilon_decay = 0.995       # facteur de d√©croissance exponentielle de epsilon\n",
        "epsilon_min = 0.01          # epsilon minimum\n",
        "for episode in range(1, num_episodes + 1):\n",
        "    state, _ = env.reset()  # r√©initialiser l'environnement\n",
        "    episode_reward = 0\n",
        "    for t in range(max_steps):\n",
        "        # S√©lectionner une action selon la politique epsilon-greedy\n",
        "        action = agent.select_action(state, epsilon)\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        episode_reward += reward\n",
        "        # Ajouter la transition dans la m√©moire\n",
        "        agent.memory.add(state, action, reward, next_state, done)\n",
        "        # Mettre √† jour l'√©tat courant\n",
        "        state = next_state\n",
        "        # Entra√Æner le r√©seau (toutes les 4 √©tapes)\n",
        "        if t % 4 == 0:\n",
        "            agent.train_step(batch_size=64, gamma=0.99, tau=1e-3)\n",
        "        # Sortir si fin d'√©pisode\n",
        "        if done:\n",
        "            break\n",
        "    # Mettre √† jour epsilon (d√©croissance exponentielle par √©pisode)\n",
        "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
        "    # Enregistrer le score de l'√©pisode\n",
        "    scores.append(episode_reward)\n",
        "    scores_window.append(episode_reward)\n",
        "    if len(scores_window) > 100:\n",
        "        # garder une fen√™tre glissante de 100 derniers √©pisodes\n",
        "        scores_window.pop(0)\n",
        "    # Calculer la r√©compense moyenne des 100 derniers √©pisodes\n",
        "    avg_reward_100 = np.mean(scores_window)\n",
        "    # Sauvegarder le mod√®le si c'est le meilleur jusqu'√† pr√©sent\n",
        "    if avg_reward_100 > best_avg_reward:\n",
        "        best_avg_reward = avg_reward_100\n",
        "        torch.save(agent.q_network.state_dict(), best_model_path)\n",
        "    # Affichage p√©riodique des statistiques d'entra√Ænement\n",
        "    if episode % print_interval == 0:\n",
        "        print(f\"√âpisode {episode}/{num_episodes} - Score moyen (100 derniers): {avg_reward_100:.1f} - eps={epsilon:.3f}\")\n",
        "    # Arr√™t anticip√© si la moyenne sur 100 √©pisodes atteint la cible\n",
        "    if avg_reward_100 >= target_score and episode >= 100:\n",
        "        print(f\"Environnement r√©solu en {episode} √©pisodes üéâ  (score moyen sur 100 eps = {avg_reward_100:.1f})\")\n",
        "        break\n",
        "\n",
        "# Fin de l'entra√Ænement\n",
        "print(\"Meilleure moyenne obtenue sur 100 √©pisodes:\", best_avg_reward)\n",
        "# Charger le meilleur mod√®le sauvegard√©\n",
        "best_model = QNetwork(state_dim, action_dim)\n",
        "best_model.load_state_dict(torch.load(best_model_path))\n",
        "best_model.eval()\n",
        "\n",
        "# √âvaluation du mod√®le entra√Æn√© sur 100 √©pisodes pour v√©rifier la performance > 200\n",
        "eval_episodes = 1000\n",
        "eval_rewards = []\n",
        "for i in range(eval_episodes):\n",
        "    state, _ = env.reset()\n",
        "    episode_sum = 0\n",
        "    while True:\n",
        "        # S√©lectionner action de fa√ßon d√©terministe (epsilon=0, politique purement optimis√©e)\n",
        "        state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            q_vals = best_model(state_t)\n",
        "        action = int(torch.argmax(q_vals, dim=1).item())\n",
        "        # Agir dans l'environnement\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        episode_sum += reward\n",
        "        state = next_state\n",
        "        if terminated or truncated:\n",
        "            eval_rewards.append(episode_sum)\n",
        "            break\n",
        "\n",
        "avg_eval_reward = np.mean(eval_rewards)\n",
        "print(f\"R√©compense moyenne sur {eval_episodes} √©pisodes d'√©valuation: {avg_eval_reward:.2f}\")\n",
        "if avg_eval_reward >= 200:\n",
        "    print(\">>> Performance cible atteinte! L'agent obtient en moyenne au-dessus de 200 ‚úÖ\")\n",
        "else:\n",
        "    print(\">>> Performance cible NON atteinte. R√©entra√Æner ou ajuster les hyperparam√®tres ‚ö†Ô∏è\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mWGRAlg8S-L"
      },
      "source": [
        "### Description\n",
        "\n",
        "This environment is a classic rocket trajectory optimization problem. According to Pontryagin‚Äôs maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
        "There are two environment versions: discrete or continuous. The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.\n",
        "\n",
        "### Action Space\n",
        "\n",
        "There are four discrete actions available:\n",
        "- 0: do nothing\n",
        "- 1: fire left orientation engine\n",
        "- 2: fire main engine\n",
        "- 3: fire right orientation engine\n",
        "\n",
        "### Observation Space\n",
        "The state is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
        "\n",
        "### Rewards\n",
        "After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
        "For each step, the reward:\n",
        "- is increased/decreased the closer/further the lander is to the landing pad.\n",
        "- is increased/decreased the slower/faster the lander is moving.\n",
        "- is decreased the more the lander is tilted (angle not horizontal).\n",
        "- is increased by 10 points for each leg that is in contact with the ground.\n",
        "- is decreased by 0.03 points each frame a side engine is firing.\n",
        "- is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
        "An episode is considered a solution if it scores at least 200 points.\n",
        "\n",
        "### Starting State\n",
        "The lander starts at the top center of the viewport with a random initial force applied to its center of mass.\n",
        "\n",
        "### Episode Termination\n",
        "The episode finishes if:\n",
        "- the lander crashes (the lander body gets in contact with the moon);\n",
        "- the lander gets outside of the viewport (x coordinate is greater than 1);\n",
        "- the lander is not awake. From the Box2D docs, a body which is not awake is a body which doesn‚Äôt move and doesn‚Äôt collide with any other body."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RsBDhoU8S-O"
      },
      "source": [
        "### Before submit\n",
        "Test that your agent has the right attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Ofi4t6Ml8S-P"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Pour g√©rer correctement l'absence potentielle de gymnasium sur la plateforme\n",
        "try:\n",
        "    import gymnasium as gym\n",
        "except ImportError:\n",
        "    print(\"gymnasium n'est pas disponible.\")\n",
        "    gym = None\n",
        "\n",
        "###################################################################\n",
        "# R√©seau de neurones QNetwork pour approximer Q(s,a)\n",
        "###################################################################\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        # R√©seau fully-connected avec 2 couches cach√©es (128 neurones chacune)\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc_out = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc_out(x)\n",
        "\n",
        "\n",
        "###################################################################\n",
        "# Buffer d'exp√©rience (ReplayBuffer)\n",
        "###################################################################\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = (\n",
        "            np.array(state, copy=True),\n",
        "            action,\n",
        "            reward,\n",
        "            np.array(next_state, copy=True),\n",
        "            done\n",
        "        )\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.memory), batch_size, replace=False)\n",
        "        states, actions, rewards, next_states, dones = zip(*(self.memory[i] for i in indices))\n",
        "        return (\n",
        "            np.array(states),\n",
        "            actions,\n",
        "            rewards,\n",
        "            np.array(next_states),\n",
        "            dones\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "\n",
        "###################################################################\n",
        "# Classe Agent (unique fichier, sans d√©pendre d'autres ressources)\n",
        "###################################################################\n",
        "class Agent:\n",
        "    def __init__(self, env, player_name=None):\n",
        "        \"\"\"\n",
        "        - env : environnement Gymnasium\n",
        "        - player_name : nom du joueur (utile en multi-agent)\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "\n",
        "        # Gestion du cas multi-agent : la m√©thode action_space(...) renvoie un action_space\n",
        "        # si c'est un environment multi-agent\n",
        "        if self.env.action_space.__class__.__name__ == \"method\":\n",
        "            # Pour un multi-agent, on sample directement\n",
        "            self.action_space = self.env.action_space(self.env.agents[0])\n",
        "            self.multi_agent = True\n",
        "        else:\n",
        "            # Environnement \"classique\" (ex: LunarLander-v2)\n",
        "            self.action_space = self.env.action_space\n",
        "            self.multi_agent = False\n",
        "\n",
        "        # Dans le cas multi-agent, on ne s'emb√™te pas : on agit al√©atoirement\n",
        "        # Dans le cas single-agent classique, on entra√Æne ici un DQN pour viser un score > 200\n",
        "        if not self.multi_agent:\n",
        "            self._train_dqn_for_lunarlander()\n",
        "\n",
        "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None, action_mask=None):\n",
        "        \"\"\"\n",
        "        S√©lectionne et renvoie une action.\n",
        "        - En multi-agent, on sample al√©atoirement (car 'method').\n",
        "        - En single-agent (LunarLander-v2), on exploite le r√©seau DQN entra√Æn√©.\n",
        "        \"\"\"\n",
        "        if self.multi_agent:\n",
        "            # multi-agent : on agit au hasard\n",
        "            action = self.action_space.sample(action_mask)\n",
        "        else:\n",
        "            # single-agent : exploite le DQN entra√Æn√©\n",
        "            state_t = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                q_values = self.q_network(state_t)\n",
        "            action = int(torch.argmax(q_values, dim=1).item())\n",
        "        return action\n",
        "\n",
        "    ###################################################################\n",
        "    # Entra√Ænement DQN en local (inclut tout le code) pour LunarLander\n",
        "    ###################################################################\n",
        "    def _train_dqn_for_lunarlander(self):\n",
        "        \"\"\"\n",
        "        Entra√Æne un agent DQN sur l'environnement self.env\n",
        "        (suppos√© √™tre un environnement Gymnasium LunarLander-v2).\n",
        "        Stocke le r√©seau entra√Æn√© dans self.q_network.\n",
        "        \"\"\"\n",
        "        # V√©rifier qu'on a gymnasium\n",
        "        if gym is None:\n",
        "            print(\"Gymnasium introuvable, impossible d'entra√Æner le DQN.\")\n",
        "            self.q_network = None\n",
        "            return\n",
        "\n",
        "        # Hyperparam√®tres\n",
        "        state_dim = self.env.observation_space.shape[0]\n",
        "        action_dim = self.env.action_space.n\n",
        "        max_episodes = 600        # nombre d'√©pisodes max (assez pour atteindre 200+)\n",
        "        max_steps = 1000          # pas max par √©pisode\n",
        "        gamma = 0.99\n",
        "        lr = 5e-4\n",
        "        batch_size = 64\n",
        "        buffer_capacity = 100000\n",
        "        update_rate = 4           # entra√Ænement du r√©seau tous les 4 pas\n",
        "        tau = 1e-3                # mise √† jour douce du r√©seau cible\n",
        "        target_score = 200        # objectif de performance\n",
        "        min_episodes_for_avg = 100\n",
        "\n",
        "        # Politique epsilon-greedy\n",
        "        epsilon = 1.0\n",
        "        epsilon_decay = 0.995\n",
        "        epsilon_min = 0.01\n",
        "\n",
        "        # R√©seaux Q principal et cible\n",
        "        self.q_network = QNetwork(state_dim, action_dim)\n",
        "        self.target_network = QNetwork(state_dim, action_dim)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.target_network.eval()\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.q_network.parameters(), lr=lr)\n",
        "        replay_buffer = ReplayBuffer(buffer_capacity)\n",
        "\n",
        "        scores_window = []\n",
        "        best_avg_reward = -float(\"inf\")\n",
        "\n",
        "        episode = 0\n",
        "        while episode < max_episodes:\n",
        "            state, _ = self.env.reset()\n",
        "            episode_reward = 0\n",
        "            for t in range(max_steps):\n",
        "                # S√©lection action epsilon-greedy\n",
        "                if random.random() < epsilon:\n",
        "                    action = self.action_space.sample()\n",
        "                else:\n",
        "                    state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "                    with torch.no_grad():\n",
        "                        q_vals = self.q_network(state_t)\n",
        "                    action = int(torch.argmax(q_vals, dim=1).item())\n",
        "\n",
        "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "                done = terminated or truncated\n",
        "                replay_buffer.add(state, action, reward, next_state, done)\n",
        "\n",
        "                state = next_state\n",
        "                episode_reward += reward\n",
        "\n",
        "                # Entra√Ænement du r√©seau\n",
        "                if t % update_rate == 0 and len(replay_buffer) >= batch_size:\n",
        "                    self._dqn_train_step(replay_buffer, batch_size, optimizer, gamma, tau)\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            # Mise √† jour epsilon\n",
        "            epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
        "\n",
        "            scores_window.append(episode_reward)\n",
        "            if len(scores_window) > 100:\n",
        "                scores_window.pop(0)\n",
        "            episode += 1\n",
        "\n",
        "            # Affichage\n",
        "            avg_last_100 = np.mean(scores_window)\n",
        "            if avg_last_100 > best_avg_reward:\n",
        "                best_avg_reward = avg_last_100\n",
        "\n",
        "            if episode % 25 == 0:\n",
        "                print(f\"[DQN training] √âpisode {episode}/{max_episodes}, \"\n",
        "                      f\"Reward: {episode_reward:.1f}, \"\n",
        "                      f\"moyenne(100): {avg_last_100:.1f}, eps={epsilon:.3f}\")\n",
        "\n",
        "            # Arr√™t si r√©solu\n",
        "            if avg_last_100 >= target_score and episode >= min_episodes_for_avg:\n",
        "                print(f\"[DQN training] Environnement r√©solu en {episode} √©pisodes, moyenne(100)={avg_last_100:.1f}!\")\n",
        "                break\n",
        "\n",
        "        print(f\"[DQN training] Meilleure moyenne(100) atteinte = {best_avg_reward:.1f}\")\n",
        "        print(\"[DQN training] Fin de l'entra√Ænement.\")\n",
        "\n",
        "    def _dqn_train_step(self, replay_buffer, batch_size, optimizer, gamma, tau):\n",
        "        \"\"\"\n",
        "        Un pas d'entra√Ænement du DQN : on √©chantillonne un mini-lot\n",
        "        et on met √† jour self.q_network et self.target_network.\n",
        "        \"\"\"\n",
        "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
        "\n",
        "        # Conversion en tenseurs PyTorch\n",
        "        states_t      = torch.tensor(states, dtype=torch.float32)\n",
        "        actions_t     = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
        "        rewards_t     = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
        "        next_states_t = torch.tensor(next_states, dtype=torch.float32)\n",
        "        dones_t       = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "        # Q-cible via le r√©seau-cible\n",
        "        with torch.no_grad():\n",
        "            q_next = self.target_network(next_states_t).max(dim=1, keepdim=True)[0]\n",
        "            q_target = rewards_t + gamma * q_next * (1 - dones_t)\n",
        "\n",
        "        # Q-pr√©sent via le r√©seau principal\n",
        "        q_current = self.q_network(states_t).gather(1, actions_t)\n",
        "\n",
        "        # Perte MSE\n",
        "        loss = F.mse_loss(q_current, q_target)\n",
        "\n",
        "        # R√©tropropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Mise √† jour douce du r√©seau cible\n",
        "        for target_param, local_param in zip(self.target_network.parameters(), self.q_network.parameters()):\n",
        "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}